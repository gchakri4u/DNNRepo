{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi5AmAYhCpmf/VGVMWWuvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gchakri4u/DNNRepo/blob/main/Skip_Gram_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "* Skip-Gram model takes a center word as input and tries to predict its surrounding/contextual words as output\n",
        "* The **first projection layer(No activation function)** is the embeddings we want to get"
      ],
      "metadata": {
        "id": "lESkxf6PrEGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8tUkO2xrCEL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Text\n",
        "#!wget https://raw.githubusercontent.com/mshossain/TextEmbeddings/refs/heads/main/sample_text.txt"
      ],
      "metadata": {
        "id": "S-WphLZesNIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  with open(file_path, 'r') as file:\n",
        "    return file.read()"
      ],
      "metadata": {
        "id": "LYHpOZ1fs7i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(input):\n",
        "  input = input.lower()\n",
        "  input = re.sub(r'[^\\w\\s]','',input) #Remove Punctuation like [.;)(\"]\n",
        "  return input.split()"
      ],
      "metadata": {
        "id": "8XJMX6O3tHUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_word_mapping(words): # word_to_id and id_to_word mapping for every unique word\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      word_to_id[word] = len(word_to_id)\n",
        "      id_to_word[len(id_to_word)] = word\n",
        "  return word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "cDckT-XRuFY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_skipgram_data(words,word_to_id,window_size):\n",
        "  output = []\n",
        "  for i in range(len(words)):\n",
        "    center_word = word_to_id[words[i]]\n",
        "    for j in range(i - window_size, i +window_size +1):\n",
        "      if j!= i and j >=0 and j < len(words):\n",
        "        context_word = word_to_id[words[j]]\n",
        "        output.append({center_word:context_word})\n",
        "  return output"
      ],
      "metadata": {
        "id": "oA2q6x3uvK7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dims):\n",
        "    super(SkipGramModel, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dims)\n",
        "    self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs): # Forward Pass internals\n",
        "    # print(\"Inputs:\", inputs)\n",
        "    # print(\"Inputs.shape:\", inputs.shape)\n",
        "    x = self.embedding(inputs) # Output shape --> (batch_size,embedding_dims)\n",
        "    return self.output_layer(x) # Output shape --> (batch_size, vocab_size)"
      ],
      "metadata": {
        "id": "UamcKeelxa2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches_with_shuffle(data, batch_size):\n",
        "  batches = []\n",
        "  np.random.shuffle(data)\n",
        "  for i in range(0, len(data), batch_size):\n",
        "    batch = data[i:i+batch_size]\n",
        "    batches.append(batch)\n",
        "  return batches"
      ],
      "metadata": {
        "id": "ng0I52hVy6nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,data,batch_size=32,epochs=10):\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    batches = create_batches_with_shuffle(data, batch_size)\n",
        "    for batch in batches:\n",
        "      center_words_batch = []\n",
        "      context_words_batch = []\n",
        "      for d in batch:\n",
        "        center_words_batch.append(list(d.keys())[0]) # Appending the first key to center_words_batch\n",
        "        context_words_batch.append(list(d.values())[0])\n",
        "      center_words_batch = tf.convert_to_tensor(center_words_batch)\n",
        "      context_words_batch = tf.one_hot(context_words_batch, depth=len(word_to_id))\n",
        "      with tf.GradientTape() as tape:\n",
        "        logits = model(center_words_batch)\n",
        "        loss = loss_fn(context_words_batch, logits)\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "      total_loss += loss\n",
        "    print(\"Epoch: {} Loss: {}\".format(epoch, total_loss))"
      ],
      "metadata": {
        "id": "JbEj6EGGzWv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper Parameters\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "embedding_dims = 15\n",
        "#-------------------\n",
        "\n",
        "# 1.Read File\n",
        "text = read_file('sample_text.txt')\n",
        "print(text)\n",
        "\n",
        "# 2.Tokenize the file data\n",
        "words = tokenizer(text)\n",
        "print(words)\n",
        "\n",
        "# 3.Create Word to Id and Id to Word Mappings\n",
        "word_to_id, id_to_word = build_word_mapping(words)\n",
        "print(\"WORD_TO_ID:\",word_to_id)\n",
        "print(\"LEN(WORD_TO_ID):\",len(word_to_id))\n",
        "\n",
        "# 4. Create Skip-Gram data {center_word,context_word}\n",
        "skipgram_data = prepare_skipgram_data(words,word_to_id,3)\n",
        "print(\"SKIPGRAM_DATA:\",skipgram_data)\n",
        "\n",
        "# 5.Create the model\n",
        "vocab_size = len(word_to_id)\n",
        "model = SkipGramModel(len(word_to_id), embedding_dims)\n",
        "\n",
        "# 6.Define optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# 7.Compile the model with optimizer and Loss Function\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "model.summary()\n",
        "\n",
        "# 8.Train the Model\n",
        "train(model,skipgram_data,batch_size,epochs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zzK2xV8G0ird",
        "outputId": "1ecc4d62-0ee9-46f2-c4a6-267d082fd184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sat on the mat.\n",
            "The dog lay on the rug.\n",
            "The cat chased the rat.\n",
            "The dog barked at the cat.\n",
            "The mat was next to the rug.\n",
            "The dog and the cat slept together on the mat.\n",
            "The mat and the rug were dirty.\n",
            "The tree is in the backyard.\n",
            "The bird flew over the trees.\n",
            "The bird sang in the tree.\n",
            "The bird liked to play near the tree.\n",
            "The tree was tall and the bird liked to sit on it.\n",
            "\n",
            "['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'lay', 'on', 'the', 'rug', 'the', 'cat', 'chased', 'the', 'rat', 'the', 'dog', 'barked', 'at', 'the', 'cat', 'the', 'mat', 'was', 'next', 'to', 'the', 'rug', 'the', 'dog', 'and', 'the', 'cat', 'slept', 'together', 'on', 'the', 'mat', 'the', 'mat', 'and', 'the', 'rug', 'were', 'dirty', 'the', 'tree', 'is', 'in', 'the', 'backyard', 'the', 'bird', 'flew', 'over', 'the', 'trees', 'the', 'bird', 'sang', 'in', 'the', 'tree', 'the', 'bird', 'liked', 'to', 'play', 'near', 'the', 'tree', 'the', 'tree', 'was', 'tall', 'and', 'the', 'bird', 'liked', 'to', 'sit', 'on', 'it']\n",
            "WORD_TO_ID: {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4, 'dog': 5, 'lay': 6, 'rug': 7, 'chased': 8, 'rat': 9, 'barked': 10, 'at': 11, 'was': 12, 'next': 13, 'to': 14, 'and': 15, 'slept': 16, 'together': 17, 'were': 18, 'dirty': 19, 'tree': 20, 'is': 21, 'in': 22, 'backyard': 23, 'bird': 24, 'flew': 25, 'over': 26, 'trees': 27, 'sang': 28, 'liked': 29, 'play': 30, 'near': 31, 'tall': 32, 'sit': 33, 'it': 34}\n",
            "LEN(WORD_TO_ID): 35\n",
            "SKIPGRAM_DATA: [{0: 1}, {0: 2}, {0: 3}, {1: 0}, {1: 2}, {1: 3}, {1: 0}, {2: 0}, {2: 1}, {2: 3}, {2: 0}, {2: 4}, {3: 0}, {3: 1}, {3: 2}, {3: 0}, {3: 4}, {3: 0}, {0: 1}, {0: 2}, {0: 3}, {0: 4}, {0: 0}, {0: 5}, {4: 2}, {4: 3}, {4: 0}, {4: 0}, {4: 5}, {4: 6}, {0: 3}, {0: 0}, {0: 4}, {0: 5}, {0: 6}, {0: 3}, {5: 0}, {5: 4}, {5: 0}, {5: 6}, {5: 3}, {5: 0}, {6: 4}, {6: 0}, {6: 5}, {6: 3}, {6: 0}, {6: 7}, {3: 0}, {3: 5}, {3: 6}, {3: 0}, {3: 7}, {3: 0}, {0: 5}, {0: 6}, {0: 3}, {0: 7}, {0: 0}, {0: 1}, {7: 6}, {7: 3}, {7: 0}, {7: 0}, {7: 1}, {7: 8}, {0: 3}, {0: 0}, {0: 7}, {0: 1}, {0: 8}, {0: 0}, {1: 0}, {1: 7}, {1: 0}, {1: 8}, {1: 0}, {1: 9}, {8: 7}, {8: 0}, {8: 1}, {8: 0}, {8: 9}, {8: 0}, {0: 0}, {0: 1}, {0: 8}, {0: 9}, {0: 0}, {0: 5}, {9: 1}, {9: 8}, {9: 0}, {9: 0}, {9: 5}, {9: 10}, {0: 8}, {0: 0}, {0: 9}, {0: 5}, {0: 10}, {0: 11}, {5: 0}, {5: 9}, {5: 0}, {5: 10}, {5: 11}, {5: 0}, {10: 9}, {10: 0}, {10: 5}, {10: 11}, {10: 0}, {10: 1}, {11: 0}, {11: 5}, {11: 10}, {11: 0}, {11: 1}, {11: 0}, {0: 5}, {0: 10}, {0: 11}, {0: 1}, {0: 0}, {0: 4}, {1: 10}, {1: 11}, {1: 0}, {1: 0}, {1: 4}, {1: 12}, {0: 11}, {0: 0}, {0: 1}, {0: 4}, {0: 12}, {0: 13}, {4: 0}, {4: 1}, {4: 0}, {4: 12}, {4: 13}, {4: 14}, {12: 1}, {12: 0}, {12: 4}, {12: 13}, {12: 14}, {12: 0}, {13: 0}, {13: 4}, {13: 12}, {13: 14}, {13: 0}, {13: 7}, {14: 4}, {14: 12}, {14: 13}, {14: 0}, {14: 7}, {14: 0}, {0: 12}, {0: 13}, {0: 14}, {0: 7}, {0: 0}, {0: 5}, {7: 13}, {7: 14}, {7: 0}, {7: 0}, {7: 5}, {7: 15}, {0: 14}, {0: 0}, {0: 7}, {0: 5}, {0: 15}, {0: 0}, {5: 0}, {5: 7}, {5: 0}, {5: 15}, {5: 0}, {5: 1}, {15: 7}, {15: 0}, {15: 5}, {15: 0}, {15: 1}, {15: 16}, {0: 0}, {0: 5}, {0: 15}, {0: 1}, {0: 16}, {0: 17}, {1: 5}, {1: 15}, {1: 0}, {1: 16}, {1: 17}, {1: 3}, {16: 15}, {16: 0}, {16: 1}, {16: 17}, {16: 3}, {16: 0}, {17: 0}, {17: 1}, {17: 16}, {17: 3}, {17: 0}, {17: 4}, {3: 1}, {3: 16}, {3: 17}, {3: 0}, {3: 4}, {3: 0}, {0: 16}, {0: 17}, {0: 3}, {0: 4}, {0: 0}, {0: 4}, {4: 17}, {4: 3}, {4: 0}, {4: 0}, {4: 4}, {4: 15}, {0: 3}, {0: 0}, {0: 4}, {0: 4}, {0: 15}, {0: 0}, {4: 0}, {4: 4}, {4: 0}, {4: 15}, {4: 0}, {4: 7}, {15: 4}, {15: 0}, {15: 4}, {15: 0}, {15: 7}, {15: 18}, {0: 0}, {0: 4}, {0: 15}, {0: 7}, {0: 18}, {0: 19}, {7: 4}, {7: 15}, {7: 0}, {7: 18}, {7: 19}, {7: 0}, {18: 15}, {18: 0}, {18: 7}, {18: 19}, {18: 0}, {18: 20}, {19: 0}, {19: 7}, {19: 18}, {19: 0}, {19: 20}, {19: 21}, {0: 7}, {0: 18}, {0: 19}, {0: 20}, {0: 21}, {0: 22}, {20: 18}, {20: 19}, {20: 0}, {20: 21}, {20: 22}, {20: 0}, {21: 19}, {21: 0}, {21: 20}, {21: 22}, {21: 0}, {21: 23}, {22: 0}, {22: 20}, {22: 21}, {22: 0}, {22: 23}, {22: 0}, {0: 20}, {0: 21}, {0: 22}, {0: 23}, {0: 0}, {0: 24}, {23: 21}, {23: 22}, {23: 0}, {23: 0}, {23: 24}, {23: 25}, {0: 22}, {0: 0}, {0: 23}, {0: 24}, {0: 25}, {0: 26}, {24: 0}, {24: 23}, {24: 0}, {24: 25}, {24: 26}, {24: 0}, {25: 23}, {25: 0}, {25: 24}, {25: 26}, {25: 0}, {25: 27}, {26: 0}, {26: 24}, {26: 25}, {26: 0}, {26: 27}, {26: 0}, {0: 24}, {0: 25}, {0: 26}, {0: 27}, {0: 0}, {0: 24}, {27: 25}, {27: 26}, {27: 0}, {27: 0}, {27: 24}, {27: 28}, {0: 26}, {0: 0}, {0: 27}, {0: 24}, {0: 28}, {0: 22}, {24: 0}, {24: 27}, {24: 0}, {24: 28}, {24: 22}, {24: 0}, {28: 27}, {28: 0}, {28: 24}, {28: 22}, {28: 0}, {28: 20}, {22: 0}, {22: 24}, {22: 28}, {22: 0}, {22: 20}, {22: 0}, {0: 24}, {0: 28}, {0: 22}, {0: 20}, {0: 0}, {0: 24}, {20: 28}, {20: 22}, {20: 0}, {20: 0}, {20: 24}, {20: 29}, {0: 22}, {0: 0}, {0: 20}, {0: 24}, {0: 29}, {0: 14}, {24: 0}, {24: 20}, {24: 0}, {24: 29}, {24: 14}, {24: 30}, {29: 20}, {29: 0}, {29: 24}, {29: 14}, {29: 30}, {29: 31}, {14: 0}, {14: 24}, {14: 29}, {14: 30}, {14: 31}, {14: 0}, {30: 24}, {30: 29}, {30: 14}, {30: 31}, {30: 0}, {30: 20}, {31: 29}, {31: 14}, {31: 30}, {31: 0}, {31: 20}, {31: 0}, {0: 14}, {0: 30}, {0: 31}, {0: 20}, {0: 0}, {0: 20}, {20: 30}, {20: 31}, {20: 0}, {20: 0}, {20: 20}, {20: 12}, {0: 31}, {0: 0}, {0: 20}, {0: 20}, {0: 12}, {0: 32}, {20: 0}, {20: 20}, {20: 0}, {20: 12}, {20: 32}, {20: 15}, {12: 20}, {12: 0}, {12: 20}, {12: 32}, {12: 15}, {12: 0}, {32: 0}, {32: 20}, {32: 12}, {32: 15}, {32: 0}, {32: 24}, {15: 20}, {15: 12}, {15: 32}, {15: 0}, {15: 24}, {15: 29}, {0: 12}, {0: 32}, {0: 15}, {0: 24}, {0: 29}, {0: 14}, {24: 32}, {24: 15}, {24: 0}, {24: 29}, {24: 14}, {24: 33}, {29: 15}, {29: 0}, {29: 24}, {29: 14}, {29: 33}, {29: 3}, {14: 0}, {14: 24}, {14: 29}, {14: 33}, {14: 3}, {14: 34}, {33: 24}, {33: 29}, {33: 14}, {33: 3}, {33: 34}, {3: 29}, {3: 14}, {3: 33}, {3: 34}, {34: 14}, {34: 33}, {34: 3}]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"skip_gram_model_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"skip_gram_model_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 64.00416564941406\n",
            "Epoch: 1 Loss: 50.62116241455078\n",
            "Epoch: 2 Loss: 44.01335906982422\n",
            "Epoch: 3 Loss: 55.26911926269531\n",
            "Epoch: 4 Loss: 91.08551025390625\n",
            "Epoch: 5 Loss: 98.94171905517578\n",
            "Epoch: 6 Loss: 97.01768493652344\n",
            "Epoch: 7 Loss: 94.09891510009766\n",
            "Epoch: 8 Loss: 92.01155090332031\n",
            "Epoch: 9 Loss: 88.90156555175781\n",
            "Epoch: 10 Loss: 78.54145812988281\n",
            "Epoch: 11 Loss: 62.859466552734375\n",
            "Epoch: 12 Loss: 63.133644104003906\n",
            "Epoch: 13 Loss: 67.70219421386719\n",
            "Epoch: 14 Loss: 63.141319274902344\n",
            "Epoch: 15 Loss: 66.74856567382812\n",
            "Epoch: 16 Loss: 52.896366119384766\n",
            "Epoch: 17 Loss: 60.88357925415039\n",
            "Epoch: 18 Loss: 68.3677978515625\n",
            "Epoch: 19 Loss: 59.66046142578125\n",
            "Epoch: 20 Loss: 67.49078369140625\n",
            "Epoch: 21 Loss: 63.42261505126953\n",
            "Epoch: 22 Loss: 67.83116149902344\n",
            "Epoch: 23 Loss: 59.167057037353516\n",
            "Epoch: 24 Loss: 60.898460388183594\n",
            "Epoch: 25 Loss: 69.54627227783203\n",
            "Epoch: 26 Loss: 60.207984924316406\n",
            "Epoch: 27 Loss: 55.99566650390625\n",
            "Epoch: 28 Loss: 72.39945983886719\n",
            "Epoch: 29 Loss: 68.03651428222656\n",
            "Epoch: 30 Loss: 65.10003662109375\n",
            "Epoch: 31 Loss: 49.33610916137695\n",
            "Epoch: 32 Loss: 68.30850982666016\n",
            "Epoch: 33 Loss: 69.73033905029297\n",
            "Epoch: 34 Loss: 61.99031066894531\n",
            "Epoch: 35 Loss: 72.13597106933594\n",
            "Epoch: 36 Loss: 67.248779296875\n",
            "Epoch: 37 Loss: 51.93505859375\n",
            "Epoch: 38 Loss: 64.73230743408203\n",
            "Epoch: 39 Loss: 61.611148834228516\n",
            "Epoch: 40 Loss: 63.790828704833984\n",
            "Epoch: 41 Loss: 66.05397033691406\n",
            "Epoch: 42 Loss: 54.01576614379883\n",
            "Epoch: 43 Loss: 51.84989929199219\n",
            "Epoch: 44 Loss: 51.70886993408203\n",
            "Epoch: 45 Loss: 51.638343811035156\n",
            "Epoch: 46 Loss: 51.567832946777344\n",
            "Epoch: 47 Loss: 51.2857666015625\n",
            "Epoch: 48 Loss: 51.920413970947266\n",
            "Epoch: 49 Loss: 51.497314453125\n",
            "Epoch: 50 Loss: 51.990936279296875\n",
            "Epoch: 51 Loss: 52.20248031616211\n",
            "Epoch: 52 Loss: 51.7088623046875\n",
            "Epoch: 53 Loss: 52.06144714355469\n",
            "Epoch: 54 Loss: 51.990936279296875\n",
            "Epoch: 55 Loss: 51.990928649902344\n",
            "Epoch: 56 Loss: 51.497314453125\n",
            "Epoch: 57 Loss: 51.92041778564453\n",
            "Epoch: 58 Loss: 51.77938461303711\n",
            "Epoch: 59 Loss: 51.84989547729492\n",
            "Epoch: 60 Loss: 51.920413970947266\n",
            "Epoch: 61 Loss: 51.708866119384766\n",
            "Epoch: 62 Loss: 51.708866119384766\n",
            "Epoch: 63 Loss: 52.13196563720703\n",
            "Epoch: 64 Loss: 51.215248107910156\n",
            "Epoch: 65 Loss: 51.990928649902344\n",
            "Epoch: 66 Loss: 52.1319694519043\n",
            "Epoch: 67 Loss: 51.77938461303711\n",
            "Epoch: 68 Loss: 51.779380798339844\n",
            "Epoch: 69 Loss: 51.920413970947266\n",
            "Epoch: 70 Loss: 51.99093246459961\n",
            "Epoch: 71 Loss: 51.779380798339844\n",
            "Epoch: 72 Loss: 52.202476501464844\n",
            "Epoch: 73 Loss: 51.84989929199219\n",
            "Epoch: 74 Loss: 51.7088623046875\n",
            "Epoch: 75 Loss: 52.20248031616211\n",
            "Epoch: 76 Loss: 51.990936279296875\n",
            "Epoch: 77 Loss: 51.567832946777344\n",
            "Epoch: 78 Loss: 51.70886993408203\n",
            "Epoch: 79 Loss: 51.63834762573242\n",
            "Epoch: 80 Loss: 51.84989929199219\n",
            "Epoch: 81 Loss: 51.63835144042969\n",
            "Epoch: 82 Loss: 51.63834762573242\n",
            "Epoch: 83 Loss: 51.56782913208008\n",
            "Epoch: 84 Loss: 51.920413970947266\n",
            "Epoch: 85 Loss: 51.426795959472656\n",
            "Epoch: 86 Loss: 51.990936279296875\n",
            "Epoch: 87 Loss: 51.497314453125\n",
            "Epoch: 88 Loss: 51.920413970947266\n",
            "Epoch: 89 Loss: 51.638343811035156\n",
            "Epoch: 90 Loss: 51.497318267822266\n",
            "Epoch: 91 Loss: 52.13196563720703\n",
            "Epoch: 92 Loss: 51.7088623046875\n",
            "Epoch: 93 Loss: 51.35627746582031\n",
            "Epoch: 94 Loss: 51.70886993408203\n",
            "Epoch: 95 Loss: 51.990928649902344\n",
            "Epoch: 96 Loss: 51.920413970947266\n",
            "Epoch: 97 Loss: 51.215248107910156\n",
            "Epoch: 98 Loss: 52.13196563720703\n",
            "Epoch: 99 Loss: 51.70886993408203\n",
            "Epoch: 100 Loss: 51.990936279296875\n",
            "Epoch: 101 Loss: 52.06145095825195\n",
            "Epoch: 102 Loss: 51.56782913208008\n",
            "Epoch: 103 Loss: 51.63834762573242\n",
            "Epoch: 104 Loss: 51.779380798339844\n",
            "Epoch: 105 Loss: 51.56782913208008\n",
            "Epoch: 106 Loss: 51.567832946777344\n",
            "Epoch: 107 Loss: 51.567832946777344\n",
            "Epoch: 108 Loss: 51.35628128051758\n",
            "Epoch: 109 Loss: 51.990928649902344\n",
            "Epoch: 110 Loss: 52.13196563720703\n",
            "Epoch: 111 Loss: 51.567832946777344\n",
            "Epoch: 112 Loss: 51.990928649902344\n",
            "Epoch: 113 Loss: 51.84989929199219\n",
            "Epoch: 114 Loss: 51.708866119384766\n",
            "Epoch: 115 Loss: 51.77937698364258\n",
            "Epoch: 116 Loss: 51.84989929199219\n",
            "Epoch: 117 Loss: 51.990928649902344\n",
            "Epoch: 118 Loss: 51.144737243652344\n",
            "Epoch: 119 Loss: 51.779380798339844\n",
            "Epoch: 120 Loss: 51.990928649902344\n",
            "Epoch: 121 Loss: 51.497318267822266\n",
            "Epoch: 122 Loss: 51.63835144042969\n",
            "Epoch: 123 Loss: 51.56783676147461\n",
            "Epoch: 124 Loss: 51.215248107910156\n",
            "Epoch: 125 Loss: 51.84989547729492\n",
            "Epoch: 126 Loss: 52.06144714355469\n",
            "Epoch: 127 Loss: 51.63834762573242\n",
            "Epoch: 128 Loss: 51.63834762573242\n",
            "Epoch: 129 Loss: 51.779380798339844\n",
            "Epoch: 130 Loss: 51.63834762573242\n",
            "Epoch: 131 Loss: 51.63835144042969\n",
            "Epoch: 132 Loss: 51.63834762573242\n",
            "Epoch: 133 Loss: 51.920413970947266\n",
            "Epoch: 134 Loss: 51.77938461303711\n",
            "Epoch: 135 Loss: 51.42679977416992\n",
            "Epoch: 136 Loss: 51.426795959472656\n",
            "Epoch: 137 Loss: 51.56782913208008\n",
            "Epoch: 138 Loss: 51.99093246459961\n",
            "Epoch: 139 Loss: 52.06145095825195\n",
            "Epoch: 140 Loss: 52.06144714355469\n",
            "Epoch: 141 Loss: 51.920413970947266\n",
            "Epoch: 142 Loss: 52.20248031616211\n",
            "Epoch: 143 Loss: 51.779380798339844\n",
            "Epoch: 144 Loss: 51.567832946777344\n",
            "Epoch: 145 Loss: 51.920413970947266\n",
            "Epoch: 146 Loss: 51.708866119384766\n",
            "Epoch: 147 Loss: 52.06144714355469\n",
            "Epoch: 148 Loss: 51.708866119384766\n",
            "Epoch: 149 Loss: 51.84989929199219\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"skip_gram_model_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"skip_gram_model_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m15\u001b[0m)                    │             \u001b[38;5;34m525\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m35\u001b[0m)                    │             \u001b[38;5;34m560\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">525</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">560</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,257\u001b[0m (12.73 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,257</span> (12.73 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,085\u001b[0m (4.24 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,085</span> (4.24 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,172\u001b[0m (8.49 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,172</span> (8.49 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embeddings from the trained model\n",
        "embeddings = model.embedding.get_weights()[0]\n",
        "\n",
        "# Print the embeddings for each word\n",
        "for word, idx in word_to_id.items():\n",
        "  print(f\"Word: {word}, Embedding: {embeddings[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvUVVLkO6Zam",
        "outputId": "752e7f00-3480-4412-c6f7-64098636e049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: the, Embedding: [ 0.10177308  0.03381066  0.09507633 -0.06960264 -0.0260177  -0.03545347\n",
            " -0.03793484  0.01664033  0.08629634 -0.01554542  0.11156334 -0.08473919\n",
            "  0.1147178   0.02996657  0.1844858 ]\n",
            "Word: cat, Embedding: [ 0.1611595   0.18163915  0.03886642 -0.08201448 -0.03473586  0.01416276\n",
            " -0.15370496  0.12088953 -0.05016429  0.01425326  0.09057663  0.14792831\n",
            "  0.0720284  -0.06700073 -0.10452331]\n",
            "Word: sat, Embedding: [ 0.1782241  -0.03263541  0.15457503  0.01060199  0.09235221 -0.00439892\n",
            " -0.1034523   0.14384605  0.08423572 -0.0361719   0.02883651  0.10898793\n",
            " -0.0173884  -0.03205472 -0.19427091]\n",
            "Word: on, Embedding: [ 0.03168623  0.06772245 -0.01003242  0.17845942 -0.27230483 -0.3767427\n",
            "  0.16985036 -0.0455696  -0.08540525 -0.08113692 -0.09503768 -0.13793829\n",
            " -0.13747314  0.13897769 -0.11141122]\n",
            "Word: mat, Embedding: [ 3.16731542e-01 -2.49721372e-04 -3.76161262e-02  1.46496668e-01\n",
            "  1.68852881e-01  3.55079286e-02 -1.24433286e-01  1.23602055e-01\n",
            " -5.15296161e-02 -1.14181021e-03  1.75091147e-01  2.65289277e-01\n",
            " -1.30086765e-01  4.81580645e-02 -2.17625752e-01]\n",
            "Word: dog, Embedding: [ 0.05766309 -0.04521151  0.09259053 -0.02555544  0.02857177  0.10383814\n",
            "  0.02691959  0.01408612  0.05306987  0.01217193 -0.02817646 -0.01467101\n",
            "  0.06242601 -0.02612649 -0.05550597]\n",
            "Word: lay, Embedding: [ 0.29025704 -0.04659234  0.12653112 -0.08635333  0.13972998  0.04417523\n",
            " -0.11447445  0.15552554 -0.02904727 -0.07325456  0.14732581  0.24145605\n",
            " -0.17229389 -0.14673351 -0.04751298]\n",
            "Word: rug, Embedding: [ 0.17166801 -0.11039357  0.00545032  0.0510993   0.2571635   0.33526817\n",
            " -0.19588998 -0.0148126  -0.17139462 -0.05248117 -0.11166569  0.19663438\n",
            " -0.0167558  -0.16921018 -0.2954822 ]\n",
            "Word: chased, Embedding: [-0.00350145 -0.07160607  0.2252897  -0.14085528 -0.02348897  0.11977196\n",
            "  0.02145628  0.17168958  0.17888436  0.12459465  0.02017207 -0.1198805\n",
            "  0.13277844 -0.05529154 -0.00760356]\n",
            "Word: rat, Embedding: [ 0.15866345 -0.01813759  0.16778457 -0.09061659  0.02200052  0.05781556\n",
            " -0.00855692  0.0417607   0.09538633  0.007974    0.02188591  0.05989412\n",
            "  0.10187859 -0.03779202 -0.0245588 ]\n",
            "Word: barked, Embedding: [ 0.09394151  0.15048303  0.10004658 -0.09119213 -0.1102018   0.00793622\n",
            " -0.02441495  0.07464192 -0.05929873 -0.10372016  0.02614333  0.09622195\n",
            "  0.0581582  -0.0221088  -0.17853034]\n",
            "Word: at, Embedding: [ 0.15485027  0.0076583   0.14510733  0.01955026 -0.07168198 -0.06417812\n",
            " -0.05222374  0.12372121  0.10566212 -0.02154658  0.06328221 -0.04525341\n",
            "  0.0929144  -0.04862567 -0.01509314]\n",
            "Word: was, Embedding: [ 0.04792133 -0.12636535  0.12392396  0.03012298 -0.02714005  0.0369823\n",
            "  0.03300131  0.22913623  0.09614788  0.06256853  0.07714383  0.16881767\n",
            " -0.13880402 -0.21210848 -0.23364493]\n",
            "Word: next, Embedding: [ 0.15913707 -0.00218074  0.03368353  0.09118687  0.02001112 -0.05305377\n",
            "  0.02205853  0.05320135 -0.04866149 -0.00967889  0.11296054  0.22534868\n",
            " -0.19596727  0.0547491  -0.19634221]\n",
            "Word: to, Embedding: [ 0.1299165   0.02526341  0.06342879 -0.01033031 -0.04978896  0.07285368\n",
            " -0.15986359  0.08518793  0.1678118  -0.02293382  0.04864181  0.0425335\n",
            "  0.04580511 -0.01006479 -0.10892365]\n",
            "Word: and, Embedding: [ 0.16842385  0.01103396  0.07271253 -0.07958355 -0.00955748  0.01394464\n",
            " -0.1620686   0.25767648  0.05724733 -0.04525732  0.17148268  0.07601451\n",
            "  0.17305222  0.02293314  0.00562639]\n",
            "Word: slept, Embedding: [ 0.09664017 -0.09443732 -0.01229578 -0.05572807  0.14049667 -0.00158588\n",
            " -0.08706439  0.08355749  0.06884521 -0.04899598  0.01460065  0.0564634\n",
            " -0.02048635 -0.04773223 -0.1207423 ]\n",
            "Word: together, Embedding: [ 0.05051797  0.07220223  0.05080194 -0.00322638 -0.15947406  0.08592997\n",
            "  0.0999625   0.00016235  0.10231986  0.05685142 -0.11855038 -0.05498455\n",
            "  0.11796594  0.07398652  0.04209705]\n",
            "Word: were, Embedding: [ 0.06277435  0.10543935  0.11233124 -0.10936614 -0.01734354  0.02156864\n",
            " -0.04381363  0.19885519  0.14719321  0.24218145  0.14089745 -0.05147703\n",
            "  0.05355408  0.08251862  0.21919334]\n",
            "Word: dirty, Embedding: [-0.01288163 -0.06345192  0.04451763 -0.23480979  0.00618731  0.15607339\n",
            " -0.01864618  0.00330963 -0.08125146  0.11312704  0.03669818  0.00463851\n",
            "  0.12373332 -0.05405579  0.05160489]\n",
            "Word: tree, Embedding: [ 0.1559491   0.18962163  0.10849731  0.2393094  -0.04984697  0.07516931\n",
            " -0.13688904  0.07199172  0.05676769 -0.0098338   0.03873578  0.12616765\n",
            " -0.04748666 -0.15386468 -0.18363324]\n",
            "Word: is, Embedding: [-0.02973866 -0.14071168  0.06224456 -0.01129814  0.11316671  0.01926906\n",
            "  0.04961368  0.05353528  0.07422891  0.02516587 -0.04373728  0.06191617\n",
            "  0.01465733 -0.18117435 -0.02122658]\n",
            "Word: in, Embedding: [ 0.05565255 -0.12042756  0.1168614  -0.10331758 -0.03344984  0.1322277\n",
            "  0.00265596  0.07090169  0.0324582   0.07764045  0.14417431  0.176211\n",
            "  0.06576793 -0.01672019  0.08148   ]\n",
            "Word: backyard, Embedding: [ 0.10300029  0.04299953  0.19257385 -0.05021704  0.01916852  0.01087748\n",
            " -0.10059783  0.20391205  0.04478633 -0.14879543  0.18491656 -0.03643088\n",
            "  0.12828675  0.01098402 -0.19434369]\n",
            "Word: bird, Embedding: [ 0.09064236 -0.03972617  0.19890773 -0.06440245  0.00399622  0.07304012\n",
            " -0.14229275 -0.01178825  0.09195647  0.04190926  0.05027346 -0.01188833\n",
            "  0.04813732  0.07368688  0.02450873]\n",
            "Word: flew, Embedding: [ 0.20213515 -0.08412437  0.04672768  0.04319628 -0.01435661  0.05937565\n",
            " -0.05402185  0.22536007  0.11116636 -0.08402053  0.09918961  0.08874131\n",
            "  0.06410497  0.06733299 -0.11870443]\n",
            "Word: over, Embedding: [ 0.09755421 -0.00695797  0.1108689  -0.05083276  0.00476635 -0.04012886\n",
            "  0.05749526  0.07434716  0.07412197 -0.08609181  0.02782014  0.00114172\n",
            "  0.00711253  0.08124922 -0.03703673]\n",
            "Word: trees, Embedding: [-0.04324569 -0.0328896   0.08796772 -0.07358052 -0.1489859   0.18791595\n",
            "  0.01514365 -0.01428975 -0.1786752   0.10856073 -0.00408898  0.01313246\n",
            " -0.0165045  -0.03989619  0.08106678]\n",
            "Word: sang, Embedding: [ 0.17790408 -0.07798035  0.16111441 -0.09949326 -0.10287412  0.08209872\n",
            " -0.18745968  0.18556224  0.02873873 -0.02513103  0.18513387  0.0310505\n",
            "  0.11369748 -0.06598347 -0.06392935]\n",
            "Word: liked, Embedding: [ 0.09107669 -0.05741197 -0.04610808 -0.08619743  0.11093562  0.07086328\n",
            " -0.18188584  0.15335068 -0.08568391 -0.14339833  0.08230142  0.01595737\n",
            " -0.05025327 -0.07838589  0.07906726]\n",
            "Word: play, Embedding: [ 0.10965365  0.02780295  0.1117254  -0.09387358 -0.0081959   0.0485788\n",
            " -0.1372291   0.10745303 -0.03978473 -0.09477341  0.0524211  -0.02032974\n",
            " -0.02012136 -0.0483667  -0.00261699]\n",
            "Word: near, Embedding: [ 0.18047422  0.08976739  0.14178951 -0.08846536  0.07550236  0.1281884\n",
            " -0.07888991  0.048923    0.04694397 -0.00895636  0.03424867  0.03502428\n",
            "  0.05434332 -0.0728322  -0.14574717]\n",
            "Word: tall, Embedding: [ 0.01488383 -0.1571826   0.04124486 -0.05930372  0.04072127  0.03839833\n",
            " -0.06294937  0.10856966 -0.08946484  0.00487594  0.08621295  0.10476451\n",
            "  0.08547529 -0.05566607 -0.00387328]\n",
            "Word: sit, Embedding: [ 0.06862898  0.02263521  0.09949112 -0.17352535 -0.31087378 -0.03807392\n",
            "  0.08944263 -0.09077932 -0.2611787  -0.01500277 -0.03402445 -0.1306589\n",
            "  0.10665051  0.16572976  0.04575798]\n",
            "Word: it, Embedding: [ 0.16786903 -0.07576089  0.0334049  -0.19686216  0.14903882 -0.11444393\n",
            " -0.078572    0.03967621 -0.05684693 -0.04909927 -0.09802557  0.08191567\n",
            " -0.01122471 -0.12865809 -0.0984473 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(word1, word2):\n",
        "  if word1 not in word_to_id or word2 not in word_to_id:\n",
        "    return None\n",
        "\n",
        "  embedding1 = embeddings[word_to_id[word1]]\n",
        "  embedding2 = embeddings[word_to_id[word2]]\n",
        "\n",
        "  dot_product = np.dot(embedding1, embedding2)\n",
        "  magnitude1 = np.linalg.norm(embedding1)\n",
        "  magnitude2 = np.linalg.norm(embedding2)\n",
        "\n",
        "  if magnitude1 == 0 or magnitude2 == 0:\n",
        "    return 0\n",
        "\n",
        "  return dot_product / (magnitude1 * magnitude2)"
      ],
      "metadata": {
        "id": "KrduIyhu7xFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"cat\"\n",
        "word2 = \"dog\"\n",
        "similarity = cosine_similarity(word1, word2)\n",
        "\n",
        "if similarity is not None:\n",
        "  print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "else:\n",
        "  print(f\"One or both of the words are not in the vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbcYK4Ex8c3W",
        "outputId": "99d310c1-384c-496d-e136-367c68f9ce1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'cat' and 'dog': 0.12253345549106598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "* https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings\n",
        "* https://medium.com/nearist-ai/word2vec-tutorial-the-skip-gram-model-c7926e1fdc09\n",
        "* https://github.com/mshossain/TextEmbeddings\n",
        "* https://medium.com/@stefanhebuaa/should-i-use-model-fit-or-tf-gradienttape-in-tensorflow-ec8664067a3"
      ],
      "metadata": {
        "id": "pce5KWj-QvzL"
      }
    }
  ]
}